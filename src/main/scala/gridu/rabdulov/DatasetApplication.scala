package gridu.rabdulov

import gridu.rabdulov.Model._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{DataTypes, StructField, StructType}


object DatasetApplication {

  def main(args: Array[String]): Unit = {

    val spark: SparkSession = SparkSession.builder
      .appName("Dataset queries")  // optional and will be autogenerated if not specified
      .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
      .config("spark.sql.warehouse.dir", "target/spark-warehouse")
//      .config("spark.hadoop.mapreduce.input.fileinputformat.input.dir.recursive","true")
      .getOrCreate

    import spark.implicits._

    val purchaseSchema = StructType(
      List(
        StructField("productCategory", DataTypes.StringType),
        StructField("productName", DataTypes.StringType),
        StructField("productPrice", DataTypes.DoubleType),
        StructField("purchaseDateTime", DataTypes.TimestampType),
        StructField("clientIp", DataTypes.StringType)
      )
    )

    val eventsDF = spark.read.schema(purchaseSchema).csv("/Users/rabdulov/Downloads/hadoop/rabdulov/events/2018/02/*")

    val purchases = eventsDF.as[Purchase]
    purchases.createOrReplaceTempView("purchase")

    val topCategories = spark.sql(
      "SELECT productCategory, count(*) cnt " +
        "FROM purchase " +
        "GROUP BY productCategory " +
        "ORDER BY cnt DESC LIMIT 10")

    println("Top Categories:")
//    //TODO send result to MySQL
    topCategories.toDF.take(10).foreach(println) //TODO why take(10) is necessary?





//    val byCategoryAndProduct = purchases.map(p => ((p.productCategory, p.productName), 1)).reduceByKey(_+_)
//    val topCategoryProducts = byCategoryAndProduct
//      .map(p => TopCategoryProducts(p._1._1, p._1._2, p._2))
//      .keyBy(p => p.productCategory)
//      .aggregateByKey(List[TopCategoryProducts]())(
//        (l, r) => (l :+ r).sortBy(-_.count).take(10),
//        (l1, l2) => (l1 ++ l2).sortBy(-_.count).take(10))
//      .values.flatMap(list => list.iterator)

//    println("Top Products per Category:")
//    //TODO send result to MySQL
//    topCategoryProducts.foreach(println)






//    val networksRDD = sc.textFile("/Users/rabdulov/Downloads/hadoop/rabdulov/GeoLite2-Country-Blocks-IPv4.csv")
//    val header1 = networksRDD.first
//    val networks = networksRDD.filter(_ != header1).map(getTokens)
//      .filter(l => !l(0).isEmpty && !l(1).isEmpty).map(TempIP.parse).keyBy(_.geonameId)
//
//    val countriesRDD = sc.textFile("/Users/rabdulov/Downloads/hadoop/rabdulov/GeoLite2-Country-Locations-en.csv")
//    val header2 = countriesRDD.first
//    val countries = countriesRDD.filter(_ != header2).map(getTokens)
//      .filter(l => !l(0).isEmpty && !l(5).isEmpty).map(TempLoc.parse).keyBy(_.geonameId)
//    countries.partitionBy(new HashPartitioner(networks.partitions.length))
//
//
//    val countryNetwork = countries.join(networks).map(j => CountryNetwork(j._2._1.country, j._2._2.network))
//
//    val byCountry = countryNetwork.keyBy(_.country).mapValues(_.network).groupByKey()
//
//    val purchaseCollection = purchases.map(p => (p.clientIp, p.productPrice)).collect()
//    sc.broadcast(purchaseCollection)
//
//    val withPurchase = byCountry.flatMapValues(_.iterator)
//      .mapValues(c => purchaseCollection.toStream
//        .filter(p => new SubnetUtils(c).getInfo.isInRange(p._1)).map(_._2).sum)
//
//    val topCountries = withPurchase.reduceByKey(_+_).sortBy(-_._2).take(10)

//    println("Top Countries:")
//    //TODO send result to MySQL
//    topCountries.foreach(println)


    println("end")
    spark.stop

  }


//  private def getTokens(value: String): Array[String] = {
//    if (!"".equals(value)) {
//      var tokens: Array[String] = csvParser.parseLine(value)
//      return tokens
//    }
//    return null
//  }

  object PurchaseEncoders {
    implicit def barEncoder: org.apache.spark.sql.Encoder[Purchase] =
      org.apache.spark.sql.Encoders.kryo[Purchase]
  }
}

